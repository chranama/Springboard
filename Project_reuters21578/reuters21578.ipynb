{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The data, once extracted, is contained within 22 files with .sgm file extension, a legacy markup format.\n",
    "#To proceed, we need to convert to .xml.  I will use the unix command osx from the library OpenSP.\n",
    "#OpenSP is not a standard library, but can be installed via the Homebrew Package Manager.\n",
    "#Homebrew is built for the macOS operating system.\n",
    "#For other environments, a different solution will need to be found to obtain OpenSP.\n",
    "\n",
    "#The installation of Homebrew was done with the following command at terminal:\n",
    "#ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\" < /dev/null 2> /dev/null\n",
    "\n",
    "#With Homebrew installed, I installed the OpenSP package with the following command at terminal:\n",
    "#brew install open-sp\n",
    "\n",
    "#Inspiration for the data representation and machine learning methods came from the following website\n",
    "#and from the corresponding kaggle competition:\n",
    "#https://towardsdatascience.com/journey-to-the-center-of-multi-label-classification-384c40229bff\n",
    "\n",
    "import requests\n",
    "import tempfile\n",
    "from os import path\n",
    "import tarfile\n",
    "import subprocess\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import numpy as np\n",
    "import collections\n",
    "from scipy.stats import t\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skmultilearn.problem_transform import BinaryRelevance, ClassifierChain, LabelPowerset\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, hamming_loss\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from skmultilearn.adapt import MLkNN\n",
    "from scipy.sparse import csr_matrix, lil_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-38c3086e0937>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'https://archive.ics.uci.edu/ml/machine-learning-databases/reuters21578-mld/reuters21578.tar.gz'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdata_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'text'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'topic_set'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mcorpus_topic_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.7/site-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'allow_redirects'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'get'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.7/site-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.7/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    531\u001b[0m         }\n\u001b[1;32m    532\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.7/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m             \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.7/site-packages/requests/models.py\u001b[0m in \u001b[0;36mcontent\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    826\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mb''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCONTENT_CHUNK_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34mb''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_content_consumed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.7/site-packages/requests/models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    748\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'stream'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 750\u001b[0;31m                     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    751\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mProtocolError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.7/site-packages/urllib3/response.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_fp_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecode_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.7/site-packages/urllib3/response.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    440\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m                 \u001b[0mcache_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 442\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    443\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Platform-specific: Buggy versions of Python.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m                     \u001b[0;31m# Close the connection when no data is returned\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    445\u001b[0m             \u001b[0;31m# Amount is given, implement using readinto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 447\u001b[0;31m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    448\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    489\u001b[0m         \u001b[0;31m# connection, and the user is reading more bytes than will be provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;31m# (for example, reading in 1k chunks)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.7/site-packages/urllib3/contrib/pyopenssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    292\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOpenSSL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSSL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSysCallError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuppress_ragged_eofs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Unexpected EOF'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.7/site-packages/OpenSSL/SSL.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1811\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSSL_peek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ssl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1812\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1813\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSSL_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ssl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1814\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_ssl_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ssl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1815\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Data acquisition\n",
    "\n",
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/reuters21578-mld/reuters21578.tar.gz'\n",
    "resp = requests.get(url)\n",
    "data_dict = {'title':[], 'text':[], 'topic_set':[]}\n",
    "corpus_topic_set = set()\n",
    "\n",
    "with tempfile.TemporaryDirectory() as tmp_dir:\n",
    "    targz_path = path.join(tmp_dir, 'reuters21578.tar.gz')\n",
    "    with open(targz_path, 'wb') as targz_file:\n",
    "        targz_file.write(resp.content)\n",
    "    with tarfile.open(targz_path, 'r:gz') as targz_file:\n",
    "        targz_file.extractall(path=tmp_dir)\n",
    "    for num in range(22):\n",
    "        sgm_path = path.join(tmp_dir, 'reut2-0' + str(num).zfill(2) + '.sgm')\n",
    "        with open(sgm_path, 'r', encoding='cp1252') as sgm_file:\n",
    "            xml_text = subprocess.run(['osx', '--directory=' + tmp_dir], stdin=sgm_file, \\\n",
    "                stdout=subprocess.PIPE, encoding='cp1252').stdout\n",
    "        soup = BeautifulSoup(xml_text, 'xml')\n",
    "        for article in soup.find_all('REUTERS'):\n",
    "            if (article.TOPICS is not None) & (article.TOPICS.D is not None) & \\\n",
    "                (article.TITLE is not None) & (article.BODY is not None):\n",
    "                topic_set = {re.sub('-', '', topic.string) for topic in article.TOPICS.find_all('D')}\n",
    "                corpus_topic_set.update(topic_set)\n",
    "                data_dict['topic_set'].append(topic_set)\n",
    "                data_dict['title'].append(article.TITLE.string)\n",
    "                data_dict['text'].append(article.BODY.string)\n",
    "for topic in corpus_topic_set:\n",
    "    data_dict[topic] = [int(topic in topic_set) for topic_set in data_dict['topic_set']]\n",
    "del data_dict['topic_set']\n",
    "data_df = pd.DataFrame(data_dict)\n",
    "binary_df = data_df.drop(columns=['title', 'text'])\n",
    "drop_columns = binary_df.columns[binary_df.sum(axis='index') <= 10].values\n",
    "data_df.drop(columns=drop_columns, inplace=True)\n",
    "data_df['topic_count'] = data_df.sum(axis='columns', numeric_only=True)\n",
    "data_df = data_df[data_df.topic_count > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exploratory Data Analysis using CountVectorizer without tuned parameters\n",
    "\n",
    "eda_text_vectorizer = CountVectorizer(strip_accents='unicode', stop_words='english')\n",
    "eda_text_matrix = eda_text_vectorizer.fit_transform(data_df.text)\n",
    "eda_topic_matrix = data_df.drop(columns=['title', 'text', 'topic_count']).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.spy(eda_topic_matrix, markersize=0.5, aspect='auto')\n",
    "plt.title('Visualization of Topic Matrix Data (Filled Cell Represents Nonzero entry)')\n",
    "plt.xlabel('Distinct Topics')\n",
    "plt.ylabel('Document')\n",
    "print('Proportion of nonzero entries: ' + str(np.count_nonzero(eda_topic_matrix)/len(eda_topic_matrix.flatten())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.spy(eda_text_matrix, markersize=0.01, aspect='auto')\n",
    "plt.title('Visualization of Text Matrix Data (Filled Cell Represents Nonzero entry)')\n",
    "plt.xlabel('Distinct Words')\n",
    "plt.ylabel('Document')\n",
    "print('Proportion of nonzero entries: ' + str(eda_text_matrix.getnnz()/len(eda_text_matrix.toarray().flatten())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(data_df.topic_count, log=True, bins=14)\n",
    "plt.title('Topics count histogram (Log scale)')\n",
    "plt.xlabel('Number of topics for an article')\n",
    "plt.ylabel('Log count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda_topic_occurence = data_df.drop(columns=['title', 'text', 'topic_count']).sum(axis='index').values\n",
    "eda_topic_percentiles = np.percentile(eda_topic_occurence, [0, 50, 75, 95, 99, 99.9, 100])\n",
    "sns.kdeplot(np.log10(eda_topic_occurence), shade=True)\n",
    "plt.title('KDE estimation of Log Topic Occurence in Corpus Distribution')\n",
    "plt.xlabel('Log Topic Occurence in Corpus')\n",
    "plt.ylabel('Probability Density')\n",
    "print('Minimum topic occurence: ' + str(eda_topic_percentiles[0]))\n",
    "print('Median topic occurence: ' + str(eda_topic_percentiles[1]))\n",
    "print('75th percentile topic occurence: ' + str(eda_topic_percentiles[2]))\n",
    "print('95th percentile topic occurence: ' + str(eda_topic_percentiles[3]))\n",
    "print('99th percentile topic occurence: ' + str(eda_topic_percentiles[4]))\n",
    "print('99.9th percentile topic occurence: ' + str(eda_topic_percentiles[5]))\n",
    "print('Maximum topic occurence: ' + str(eda_topic_percentiles[6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda_word_occurence = np.count_nonzero(eda_text_matrix.toarray(), axis=0)\n",
    "eda_word_percentiles = np.percentile(eda_word_occurence, [0, 50, 75, 95, 99, 99.9, 100])\n",
    "sns.kdeplot(np.log10(eda_word_occurence), shade=True)\n",
    "plt.title('KDE estimation of Log Word Occurence in Corpus Cumulative Distribution')\n",
    "plt.xlabel('Log Word Occurence in Corpus')\n",
    "plt.ylabel('Probability Density')\n",
    "print('Minimum word occurence: ' + str(eda_word_percentiles[0]))\n",
    "print('Median word occurence: ' + str(eda_word_percentiles[1]))\n",
    "print('75th percentile word occurence: ' + str(eda_word_percentiles[2]))\n",
    "print('95th percentile word occurence: ' + str(eda_word_percentiles[3]))\n",
    "print('99th percentile word occurence: ' + str(eda_word_percentiles[4]))\n",
    "print('99.9th percentile word occurence: ' + str(eda_word_percentiles[5]))\n",
    "print('Maximum word occurence: ' + str(eda_word_percentiles[6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical Inference\n",
    "\n",
    "single_topic_df = data_df[data_df.topic_count == 1].reset_index(drop=True).drop(columns='topic_count')\n",
    "st_binary_df = single_topic_df.drop(columns=['title', 'text'])\n",
    "st_drop_columns = st_binary_df.columns[st_binary_df.sum(axis='index') <= 10].values\n",
    "single_topic_df.drop(columns=st_drop_columns, inplace=True)\n",
    "single_topic_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def centroid(matrix):\n",
    "    m, n = matrix.shape\n",
    "    cent = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        cent[i] = matrix[:,i].mean()\n",
    "    return cent\n",
    "def square_distance(vector_1, vector_2):\n",
    "    sq_dist = 0\n",
    "    for i in range(len(vector_1)):\n",
    "        sq_dist += (vector_2[i] - vector_1[i]) ** 2\n",
    "    return sq_dist\n",
    "def closest_distance(matrix):\n",
    "    m, n = matrix.shape\n",
    "    pairwise_distance = np.zeros((m,m))\n",
    "    for i in range(m):\n",
    "        for j in range(i+1,m):\n",
    "            pairwise_distance[i,j] = square_distance(matrix[i], matrix[j])\n",
    "    return np.unravel_index(np.argmax(pairwise_distance), pairwise_distance.shape)\n",
    "def closest_centroids_inf(inf_data_df, inf_vectorizer):\n",
    "    inf_vectorizer.fit(inf_data_df.text)\n",
    "    topics = inf_data_df.drop(columns=['title','text']).columns.values\n",
    "    inf_dict = {'topic':[], 'topic_matrix':[], 'topic_centroid':[]}\n",
    "    for topic in topics:\n",
    "        inf_dict['topic'].append(topic)\n",
    "        mask = inf_data_df[topic].astype('bool')\n",
    "        topic_matrix = inf_vectorizer.transform(inf_data_df[mask].text).toarray()\n",
    "        inf_dict['topic_centroid'].append(centroid(topic_matrix))\n",
    "        inf_dict['topic_matrix'].append(topic_matrix)\n",
    "    inf_df = pd.DataFrame(inf_dict)\n",
    "    centroids = np.concatenate(inf_df.topic_centroid.values).reshape((len(inf_df.index),\\\n",
    "        len(inf_df.loc[0,'topic_centroid'])))\n",
    "    index_1, index_2 = closest_distance(centroids)\n",
    "    topic_1 = inf_df.loc[index_1, 'topic']\n",
    "    topic_2 = inf_df.loc[index_2, 'topic']\n",
    "    topic_1_matrix = inf_df.loc[index_1, 'topic_matrix']\n",
    "    topic_2_matrix = inf_df.loc[index_2, 'topic_matrix']\n",
    "    topic_1_centroid = inf_df.loc[index_1, 'topic_centroid']\n",
    "    topic_2_centroid = inf_df.loc[index_2, 'topic_centroid']\n",
    "    topic_distance = np.sqrt(square_distance(topic_1_centroid, topic_2_centroid))\n",
    "    topic_1_doc_count, _ = topic_1_matrix.shape\n",
    "    topic_2_doc_count, _ = topic_2_matrix.shape\n",
    "    topic_1_variance = 0\n",
    "    topic_2_variance = 0\n",
    "    for doc in range(topic_1_doc_count):\n",
    "        topic_1_variance += square_distance(topic_1_matrix[doc], topic_1_centroid)\n",
    "    for doc in range(topic_2_doc_count):\n",
    "        topic_2_variance += square_distance(topic_2_matrix[doc], topic_2_centroid)\n",
    "    topic_1_variance *= 1 / (topic_1_doc_count - 1)\n",
    "    topic_2_variance *= 1 / (topic_2_doc_count - 1)\n",
    "    degrees_of_freedom = topic_1_doc_count + topic_2_doc_count - 2\n",
    "    standard_error = np.sqrt(((topic_1_doc_count - 1) * topic_1_variance + (topic_2_doc_count - 1) \\\n",
    "        * topic_2_variance) / degrees_of_freedom * (1 / topic_1_doc_count + 1 / topic_2_doc_count))\n",
    "    t_stat = topic_distance / standard_error\n",
    "    p_val = 1 - t.cdf(t_stat, degrees_of_freedom)\n",
    "    return topic_1, topic_2, t_stat, p_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "inf_count_vectorizer = CountVectorizer(strip_accents='unicode', stop_words='english')\n",
    "topic_1_count, topic_2_count, t_stat_count, p_val_count = closest_centroids_inf(single_topic_df, inf_count_vectorizer)\n",
    "print('The closest 2 topics in our count vector space were: ', topic_1_count, topic_2_count)\n",
    "print('The t statistic for the difference of topic centroids in the count space was: ', t_stat_count, '\\n', \\\n",
    "   ' with associated p value: ', p_val_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "inf_bin_vectorizer = CountVectorizer(strip_accents='unicode', stop_words='english', binary=True)\n",
    "topic_1_bin, topic_2_bin, t_stat_bin, p_val_bin = closest_centroids_inf(single_topic_df, inf_bin_vectorizer)\n",
    "print('The closest 2 topics in our binary vector space were: ', topic_1_bin, topic_2_bin)\n",
    "print('The t statistic for the difference of topic centroids in the binary space was: ', t_stat_bin, '\\n', \\\n",
    "   ' with associated p value: ', p_val_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "inf_tfidf_vectorizer = TfidfVectorizer(strip_accents='unicode', stop_words='english')\n",
    "topic_1_tfidf, topic_2_tfidf, t_stat_tfidf, p_val_tfidf = closest_centroids_inf(single_topic_df, inf_tfidf_vectorizer)\n",
    "print('The closest 2 topics in our tfidf vector space were: ', topic_1_tfidf, topic_2_tfidf)\n",
    "print('The t statistic for the difference of topic centroids in the tfidf space was: ', t_stat_tfidf, '\\n', \\\n",
    "   ' with associated p value: ', p_val_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multilabel Classification\n",
    "\n",
    "double_topic_df = data_df[data_df.topic_count == 2].reset_index(drop=True).drop(columns='topic_count')\n",
    "dt_binary_df = double_topic_df.drop(columns=['text', 'title'])\n",
    "st_common_topics = st_binary_df.sum(axis='index').sort_values(ascending=False).head(n=5).index.values\n",
    "dt_common_topics = dt_binary_df.sum(axis='index').sort_values(ascending=False).head(n=5).index.values\n",
    "common_topics = np.union1d(st_common_topics, dt_common_topics)\n",
    "simple_df = data_df[np.append(np.array(['title', 'text']), common_topics)]\n",
    "simple_df = simple_df[simple_df.sum(axis='columns', numeric_only=True) > 0]\n",
    "\n",
    "smpl_train_df, smpl_test_df = train_test_split(simple_df, random_state=1)\n",
    "smpl_clf_text_vectorizer = TfidfVectorizer(strip_accents='unicode', stop_words='english')\n",
    "smpl_clf_text_vectorizer.fit(simple_df.text)\n",
    "smpl_x_train = smpl_clf_text_vectorizer.transform(smpl_train_df.text)\n",
    "smpl_y_train = smpl_train_df.drop(columns=['title', 'text'])\n",
    "smpl_x_test = smpl_clf_text_vectorizer.transform(smpl_test_df.text)\n",
    "smpl_y_test = smpl_test_df.drop(columns=['title', 'text'])\n",
    "\n",
    "train_df, test_df = train_test_split(data_df, random_state=1)\n",
    "clf_text_vectorizer = TfidfVectorizer(strip_accents='unicode', stop_words='english')\n",
    "clf_text_vectorizer.fit(data_df.text)\n",
    "x_train = clf_text_vectorizer.transform(train_df.text)\n",
    "y_train = train_df.drop(columns=['title', 'text'])\n",
    "x_test = clf_text_vectorizer.transform(test_df.text)\n",
    "y_test = test_df.drop(columns=['title', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print('Binary Relevance with Gaussian Naive Bayes (simplified dataset)', '\\n')\n",
    "brgnb_smpl_classifier = BinaryRelevance(GaussianNB())\n",
    "brgnb_smpl_classifier.fit(smpl_x_train, smpl_y_train)\n",
    "brgnb_smpl_y_pred = brgnb_smpl_classifier.predict(smpl_x_test)\n",
    "print('Test Accuracy: ', accuracy_score(smpl_y_test, brgnb_smpl_y_pred))\n",
    "print('Test Hamming loss: ', hamming_loss(smpl_y_test, brgnb_smpl_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print('One vs Rest classifier with Logistic Regression (simplified dataset)', '\\n')\n",
    "logreg_smpl_pipeline = Pipeline([('clf', OneVsRestClassifier(LogisticRegression(solver='sag'), n_jobs=-1)),])\n",
    "for topic in common_topics:\n",
    "    logreg_smpl_pipeline.fit(smpl_x_train, smpl_y_train[topic])\n",
    "    lrp_smpl_y_pred = logreg_smpl_pipeline.predict(x_test)\n",
    "    print('Test Accuracy for topic ' + topic + ': ' + str(accuracy_score(smpl_y_test[topic], lrp_smpl_y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print('Classifier Chain with Logistic Regression (simplified dataset)', '\\n')\n",
    "cclr_smpl_classifier = ClassifierChain(LogisticRegression())\n",
    "cclr_smpl_classifier.fit(smpl_x_train, smpl_y_train)\n",
    "cclr_smpl_y_pred = cclr_smpl_classifier.predict(smpl_x_test)\n",
    "print('Test Accuracy: ' + str(accuracy_score(smpl_y_test, cclr_smpl_y_pred)))\n",
    "print('Test Hamming Loss: ' + str(hamming_loss(smpl_y_test, cclr_smpl_y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print('Label Powerset with Logistic Regression (simplified dataset)', '\\n')\n",
    "lplr_smpl_classifier = LabelPowerset(LogisticRegression(solver='sag'))\n",
    "lplr_smpl_classifier.fit(smpl_x_train, smpl_y_train)\n",
    "lplr_smpl_y_pred = lplr_smpl_classifier.predict(smpl_x_test)\n",
    "print('Test Accuracy: ' + str(accuracy_score(smpl_y_test, lplr_smpl_y_pred)))\n",
    "print('Test Hamming Loss: ' + str(hamming_loss(smpl_y_test, lplr_smpl_y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print('Binary Relevance with Logistic Regression (simplified dataset)', '\\n')\n",
    "brlr_smpl_classifier = BinaryRelevance(LogisticRegression())\n",
    "brlr_smpl_classifier.fit(smpl_x_train, smpl_y_train)\n",
    "brlr_smpl_y_pred = brlr_smpl_classifier.predict(smpl_x_test)\n",
    "print('Test Accuracy: ', accuracy_score(smpl_y_test, brlr_smpl_y_pred))\n",
    "print('Test Hamming loss: ', hamming_loss(smpl_y_test, brlr_smpl_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('Multi-label k-Nearest Neighbors', '\\n')\n",
    "#x_train_dense = lil_matrix(x_train).toarray()\n",
    "#y_train_dense = lil_matrix(y_train).toarray()\n",
    "#x_test_dense = lil_matrix(x_test).toarray()\n",
    "#mlknn_classifier = MLkNN(k=10)\n",
    "#mlknn_classifier.fit(x_train_dense, y_train_dense)\n",
    "#mlknn_y_pred = mlknn_classifer.predict(x_test_dense)\n",
    "#print('Test Accuracy: ' + str(accuracy_score(y_test, mlknn_y_pred)))\n",
    "#print('Test Hamming Loss: ' + str(hamming_loss(y_test, mlknn_y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#print('Binary Relevance with Gaussian Naive Bayes', '\\n')\n",
    "#brgnb_classifier = BinaryRelevance(GaussianNB())\n",
    "#brgnb_classifier.fit(x_train, y_train)\n",
    "#brgnb_y_pred = brgnb_classifier.predict(x_test)\n",
    "#print('Test Accuracy: ', accuracy_score(y_test, brgnb_y_pred))\n",
    "#print('Test Hamming loss: ', hamming_loss(y_test, brgnb_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print('One vs Rest classifier with Logistic Regression', '\\n')\n",
    "logreg_pipeline = Pipeline([('clf', OneVsRestClassifier(LogisticRegression(solver='sag'), n_jobs=-1))])\n",
    "for topic in data_df.drop(columns=['title', 'text']).columns:\n",
    "    logreg_pipeline.fit(x_train, y_train[topic])\n",
    "    lrp_y_pred = logreg_pipeline.predict(x_test)\n",
    "    print('Test Accuracy for topic ' + topic + ': ' + str(accuracy_score(y_test[topic], lrp_y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print('Classifier Chain with Logistic Regression', '\\n')\n",
    "cclr_classifier = ClassifierChain(LogisticRegression())\n",
    "cclr_classifier.fit(x_train, y_train)\n",
    "cclr_y_pred = cclr_classifier.predict(x_test)\n",
    "print('Test Accuracy: ' + str(accuracy_score(y_test, cclr_y_pred)))\n",
    "print('Test Hamming Loss: ' + str(hamming_loss(y_test, cclr_y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print('Label Powerset with Logistic Regression', '\\n')\n",
    "lplr_classifier = LabelPowerset(LogisticRegression())\n",
    "lplr_classifier.fit(x_train, y_train)\n",
    "lplr_y_pred = lplr_classifier.predict(x_test)\n",
    "print('Test Accuracy: ' + str(accuracy_score(y_test, lplr_y_pred)))\n",
    "print('Test Hamming Loss: ' + str(hamming_loss(y_test, lplr_y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print('Binary Relevance with Logistic Regression', '\\n')\n",
    "brlr_classifier = BinaryRelevance(LogisticRegression())\n",
    "brlr_classifier.fit(x_train, y_train)\n",
    "brlr_y_pred = brlr_classifier.predict(x_test)\n",
    "print('Test Accuracy: ', accuracy_score(y_test, brlr_y_pred))\n",
    "print('Test Hamming loss: ', hamming_loss(y_test, brlr_y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
